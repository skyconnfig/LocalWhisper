# Whisper App AIæœåŠ¡é›†æˆæ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† Whisper App çš„AIæœåŠ¡é›†æˆæ¶æ„ï¼ŒåŒ…æ‹¬æœ¬åœ°Whisperè¯­éŸ³è¯†åˆ«ã€Ollamaå¤§è¯­è¨€æ¨¡å‹å’Œç›¸å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“‹ ç›®å½•

1. [AIæœåŠ¡æ¦‚è§ˆ](#AIæœåŠ¡æ¦‚è§ˆ)
2. [Whisperè¯­éŸ³è¯†åˆ«](#Whisperè¯­éŸ³è¯†åˆ«)
3. [Ollamaå¤§è¯­è¨€æ¨¡å‹](#Ollamaå¤§è¯­è¨€æ¨¡å‹)
4. [æœåŠ¡é›†æˆæ¶æ„](#æœåŠ¡é›†æˆæ¶æ„)
5. [æ¨¡å‹ç®¡ç†](#æ¨¡å‹ç®¡ç†)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [é”™è¯¯å¤„ç†å’Œå®¹é”™](#é”™è¯¯å¤„ç†å’Œå®¹é”™)
8. [ç›‘æ§å’Œè¯Šæ–­](#ç›‘æ§å’Œè¯Šæ–­)
9. [æ‰©å±•å’Œå®šåˆ¶](#æ‰©å±•å’Œå®šåˆ¶)

## ğŸ¤– AIæœåŠ¡æ¦‚è§ˆ

Whisper App é›†æˆäº†ä¸¤ä¸ªæ ¸å¿ƒçš„AIæœåŠ¡ï¼š

### æœåŠ¡æ¶æ„
```
Audio Input â†’ Whisper Service â†’ Text Output â†’ LLM Service â†’ Processed Output
     â†“              â†“                â†“           â†“            â†“
  éŸ³é¢‘æ–‡ä»¶        è¯­éŸ³è¯†åˆ«         è½¬å½•æ–‡æœ¬      æ–‡æœ¬å¤„ç†      æ™ºèƒ½è¾“å‡º
```

### æŠ€æœ¯é€‰å‹

| æœåŠ¡ | æŠ€æœ¯ | ç‰ˆæœ¬ | ç”¨é€” | ä¼˜åŠ¿ |
|------|------|------|------|------|
| **è¯­éŸ³è¯†åˆ«** | OpenAI Whisper | latest | éŸ³é¢‘è½¬æ–‡å­— | é«˜å‡†ç¡®ç‡ã€å¤šè¯­è¨€æ”¯æŒ |
| **å¤§è¯­è¨€æ¨¡å‹** | Ollama | latest | æ–‡æœ¬å¤„ç† | æœ¬åœ°éƒ¨ç½²ã€éšç§ä¿æŠ¤ |
| **éŸ³é¢‘å¤„ç†** | FFmpeg | ç³»ç»Ÿå®‰è£… | éŸ³é¢‘é¢„å¤„ç† | æ ¼å¼è½¬æ¢ã€è´¨é‡ä¼˜åŒ– |

### éƒ¨ç½²æ¨¡å¼
- **å®Œå…¨æœ¬åœ°åŒ–**: æ‰€æœ‰AIæ¨ç†åœ¨æœ¬åœ°æ‰§è¡Œ
- **æ— æ•°æ®å¤–ä¼ **: éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®ä¸ç¦»å¼€ç”¨æˆ·ç¯å¢ƒ
- **ç¦»çº¿å·¥ä½œ**: æ–­ç½‘æƒ…å†µä¸‹ä»å¯æ­£å¸¸ä½¿ç”¨
- **GPUåŠ é€Ÿ**: æ”¯æŒNVIDIA GPUåŠ é€Ÿæ¨ç†

## ğŸ¤ Whisperè¯­éŸ³è¯†åˆ«

### æœåŠ¡é…ç½®

**Dockerå®¹å™¨é…ç½®**:
```yaml
# docker-compose.yml
ollama:
  image: ollama/ollama:latest
  container_name: whisper_ollama
  restart: unless-stopped
  ports:
    - "11434:11434"
  environment:
    - OLLAMA_HOST=0.0.0.0
  volumes:
    - ollama_data:/root/.ollama
  networks:
    - whisper_network
```

**ç¯å¢ƒå˜é‡é…ç½®**:
```bash
# .env.local
OLLAMA_BASE_URL="http://localhost:11434"
LOCAL_WHISPER_ENABLED=true
WHISPER_MODEL="whisper:latest"
```

### Whisperæ¨¡å‹è§„æ ¼

| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | é€Ÿåº¦ | å‡†ç¡®æ€§ | é€‚ç”¨åœºæ™¯ |
|------|-------|----------|------|--------|----------|
| `tiny` | 39M | ~1GB | æœ€å¿« | ä¸€èˆ¬ | å¿«é€Ÿæµ‹è¯•ã€èµ„æºå—é™ |
| `base` | 74M | ~1GB | å¿« | è‰¯å¥½ | æ—¥å¸¸ä½¿ç”¨æ¨è |
| `small` | 244M | ~2GB | ä¸­ç­‰ | å¾ˆå¥½ | å¹³è¡¡æ€§èƒ½å’Œè´¨é‡ |
| `medium` | 769M | ~5GB | æ…¢ | ä¼˜ç§€ | é«˜è´¨é‡éœ€æ±‚ |
| `large-v3` | 1550M | ~10GB | æœ€æ…¢ | æœ€ä½³ | ä¸“ä¸šçº§åº”ç”¨ |

### æ ¸å¿ƒæœåŠ¡å®ç°

**LocalWhisperServiceç±»**:
```typescript
// lib/localWhisperService.ts
export class LocalWhisperService {
  private baseUrl: string;
  private model: string;

  constructor() {
    this.baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    this.model = process.env.WHISPER_MODEL || 'whisper:latest';
  }

  async transcribe(audioUrl: string, options?: TranscribeOptions): Promise<TranscribeResult> {
    try {
      // 1. ä¸‹è½½å’Œé¢„å¤„ç†éŸ³é¢‘
      const audioBuffer = await this.downloadAudio(audioUrl);
      const processedAudio = await this.preprocessAudio(audioBuffer);

      // 2. è°ƒç”¨Whisperæ¨¡å‹
      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: this.model,
          prompt: this.buildTranscriptionPrompt(options),
          images: [processedAudio.toString('base64')],
          stream: false
        })
      });

      // 3. å¤„ç†å“åº”
      const result = await response.json();
      return this.parseTranscriptionResult(result);

    } catch (error) {
      throw new WhisperServiceError(`Transcription failed: ${error.message}`);
    }
  }

  private async downloadAudio(url: string): Promise<Buffer> {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`Failed to download audio: ${response.statusText}`);
    }
    return Buffer.from(await response.arrayBuffer());
  }

  private async preprocessAudio(buffer: Buffer): Promise<Buffer> {
    // ä½¿ç”¨FFmpegè¿›è¡ŒéŸ³é¢‘é¢„å¤„ç†
    return new Promise((resolve, reject) => {
      const ffmpeg = spawn('ffmpeg', [
        '-i', 'pipe:0',          // ä»stdinè¯»å–
        '-ar', '16000',          // é‡‡æ ·ç‡16kHz
        '-ac', '1',              // å•å£°é“
        '-c:a', 'pcm_s16le',     // PCM 16ä½
        '-f', 'wav',             // WAVæ ¼å¼
        'pipe:1'                 // è¾“å‡ºåˆ°stdout
      ]);

      const chunks: Buffer[] = [];
      
      ffmpeg.stdout.on('data', (chunk) => chunks.push(chunk));
      ffmpeg.stdout.on('end', () => resolve(Buffer.concat(chunks)));
      ffmpeg.stderr.on('data', (data) => console.error('FFmpeg:', data.toString()));
      ffmpeg.on('error', reject);

      ffmpeg.stdin.write(buffer);
      ffmpeg.stdin.end();
    });
  }
}
```

### è½¬å½•é€‰é¡¹é…ç½®

```typescript
interface TranscribeOptions {
  language?: string;           // æŒ‡å®šè¯­è¨€ä»£ç  (å¦‚ 'zh', 'en')
  task?: 'transcribe' | 'translate';  // ä»»åŠ¡ç±»å‹
  temperature?: number;        // éšæœºæ€§æ§åˆ¶ (0-1)
  initial_prompt?: string;     // åˆå§‹æç¤º
  vad_filter?: boolean;        // è¯­éŸ³æ´»åŠ¨æ£€æµ‹
  no_speech_threshold?: number; // é™éŸ³é˜ˆå€¼
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await whisperService.transcribe(audioUrl, {
  language: 'zh',
  temperature: 0.1,
  vad_filter: true,
  initial_prompt: 'è¿™æ˜¯ä¸€æ®µä¸­æ–‡å½•éŸ³'
});
```

### è¯­è¨€æ”¯æŒ

**æ”¯æŒçš„è¯­è¨€åˆ—è¡¨**:
```typescript
export const SUPPORTED_LANGUAGES = {
  'auto': 'è‡ªåŠ¨æ£€æµ‹',
  'zh': 'ä¸­æ–‡',
  'en': 'English',
  'es': 'EspaÃ±ol',
  'fr': 'FranÃ§ais',
  'de': 'Deutsch',
  'ja': 'æ—¥æœ¬èª',
  'ko': 'í•œêµ­ì–´',
  'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
  'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
  'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€',
  // ... æ›´å¤šè¯­è¨€
} as const;

export type LanguageCode = keyof typeof SUPPORTED_LANGUAGES;
```

## ğŸ§  Ollamaå¤§è¯­è¨€æ¨¡å‹

### æ¨¡å‹ç®¡ç†

**æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨**:
```bash
# ä¸‹è½½æ¨èæ¨¡å‹
docker exec whisper_ollama ollama pull llama3.1:8b        # é€šç”¨æ¨¡å‹
docker exec whisper_ollama ollama pull qwen2:7b           # ä¸­æ–‡ä¼˜åŒ–
docker exec whisper_ollama ollama pull codellama:7b       # ä»£ç å¤„ç†
docker exec whisper_ollama ollama pull mistral:7b         # è½»é‡é«˜æ•ˆ
```

**æ¨¡å‹é…ç½®å¯¹æ¯”**:
| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | ä¸­æ–‡æ”¯æŒ | é€‚ç”¨åœºæ™¯ |
|------|-------|----------|----------|----------|
| `llama3.1:8b` | 8B | ~8GB | è‰¯å¥½ | é€šç”¨æ–‡æœ¬å¤„ç† |
| `qwen2:7b` | 7B | ~7GB | ä¼˜ç§€ | ä¸­æ–‡æ–‡æœ¬å¤„ç† |
| `mistral:7b` | 7B | ~7GB | ä¸€èˆ¬ | é«˜æ•ˆæ¨ç† |
| `codellama:7b` | 7B | ~7GB | ä¸€èˆ¬ | ä»£ç ç”Ÿæˆåˆ†æ |

### LLMæœåŠ¡å®ç°

**LocalLLMServiceç±»**:
```typescript
// lib/localLLMService.ts
export class LocalLLMService {
  private baseUrl: string;
  private defaultModel: string;

  constructor() {
    this.baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    this.defaultModel = process.env.LOCAL_LLM_MODEL || 'llama3.1:8b';
  }

  async generate(prompt: string, options?: GenerateOptions): Promise<string> {
    const response = await fetch(`${this.baseUrl}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: options?.model || this.defaultModel,
        prompt: this.buildPrompt(prompt, options),
        stream: false,
        options: {
          temperature: options?.temperature || 0.7,
          top_p: options?.top_p || 0.9,
          max_tokens: options?.max_tokens || 2000,
        }
      })
    });

    const result = await response.json();
    return result.response;
  }

  async generateStream(prompt: string, options?: GenerateOptions): Promise<AsyncIterableIterator<string>> {
    const response = await fetch(`${this.baseUrl}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: options?.model || this.defaultModel,
        prompt: this.buildPrompt(prompt, options),
        stream: true,
        options: {
          temperature: options?.temperature || 0.7,
          top_p: options?.top_p || 0.9,
          max_tokens: options?.max_tokens || 2000,
        }
      })
    });

    return this.parseStreamResponse(response);
  }

  private buildPrompt(userPrompt: string, options?: GenerateOptions): string {
    const systemPrompt = options?.systemPrompt || 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚';
    
    return `<|begin_of_text|><|start_header_id|>system<|end_header_id|>
${systemPrompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
${userPrompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>`;
  }

  private async *parseStreamResponse(response: Response): AsyncIterableIterator<string> {
    if (!response.body) throw new Error('No response body');

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());

        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            if (data.response) {
              yield data.response;
            }
            if (data.done) {
              return;
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }
}
```

### æç¤ºæ¨¡æ¿ç³»ç»Ÿ

**é¢„å®šä¹‰æç¤ºæ¨¡æ¿**:
```typescript
export const PROMPT_TEMPLATES = {
  summary: `è¯·ä¸ºä»¥ä¸‹æ–‡å­—ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼š

{transcription}

æ‘˜è¦ï¼š`,

  email: `è¯·å°†ä»¥ä¸‹è¯­éŸ³è½¬å½•å†…å®¹è½¬æ¢ä¸ºæ­£å¼çš„é‚®ä»¶æ ¼å¼ï¼š

è½¬å½•å†…å®¹ï¼š{transcription}

é‚®ä»¶ï¼š`,

  meeting_notes: `è¯·å°†ä»¥ä¸‹ä¼šè®®è½¬å½•æ•´ç†ä¸ºç»“æ„åŒ–çš„ä¼šè®®çºªè¦ï¼š

è½¬å½•å†…å®¹ï¼š{transcription}

ä¼šè®®çºªè¦ï¼š
1. ä¸»è¦è®®é¢˜ï¼š
2. å…³é”®å†³å®šï¼š
3. è¡ŒåŠ¨é¡¹ç›®ï¼š
4. ä¸‹æ¬¡ä¼šè®®ï¼š`,

  action_items: `è¯·ä»ä»¥ä¸‹è½¬å½•å†…å®¹ä¸­æå–è¡ŒåŠ¨é¡¹ç›®å’Œå¾…åŠäº‹é¡¹ï¼š

è½¬å½•å†…å®¹ï¼š{transcription}

è¡ŒåŠ¨é¡¹ç›®ï¼š`,

  custom: `{prompt}

è½¬å½•å†…å®¹ï¼š{transcription}

å›ç­”ï¼š`
} as const;

export type PromptType = keyof typeof PROMPT_TEMPLATES;
```

## ğŸ”§ æœåŠ¡é›†æˆæ¶æ„

### æœåŠ¡äº¤äº’æµç¨‹

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant API as APIç½‘å…³
    participant WhisperSvc as WhisperæœåŠ¡
    participant LLMSvc as LLMæœåŠ¡
    participant Storage as å­˜å‚¨æœåŠ¡
    participant DB as æ•°æ®åº“

    Client->>API: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    API->>Storage: ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    Storage-->>API: è¿”å›æ–‡ä»¶URL
    
    API->>WhisperSvc: å‘èµ·è½¬å½•è¯·æ±‚
    WhisperSvc->>Storage: ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
    WhisperSvc->>WhisperSvc: æ‰§è¡Œè¯­éŸ³è¯†åˆ«
    WhisperSvc-->>API: è¿”å›è½¬å½•ç»“æœ
    
    API->>DB: ä¿å­˜è½¬å½•è®°å½•
    API-->>Client: è¿”å›è½¬å½•ç»“æœ
    
    Client->>API: è¯·æ±‚AIå¤„ç†
    API->>LLMSvc: å‘é€å¤„ç†è¯·æ±‚
    LLMSvc->>LLMSvc: æ‰§è¡Œæ–‡æœ¬å¤„ç†
    LLMSvc-->>API: æµå¼è¿”å›ç»“æœ
    
    API->>DB: ä¿å­˜å¤„ç†ç»“æœ
    API-->>Client: æµå¼è¿”å›ç»“æœ
```

### é›†æˆæœåŠ¡ç±»

**AIServiceManager**:
```typescript
// lib/aiServiceManager.ts
export class AIServiceManager {
  private whisperService: LocalWhisperService;
  private llmService: LocalLLMService;
  private cache: Redis;

  constructor() {
    this.whisperService = new LocalWhisperService();
    this.llmService = new LocalLLMService();
    this.cache = new Redis(process.env.REDIS_URL);
  }

  async transcribeAudio(audioUrl: string, options?: TranscribeOptions): Promise<TranscribeResult> {
    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = this.generateCacheKey('transcribe', audioUrl, options);
    const cached = await this.cache.get(cacheKey);
    if (cached) {
      return JSON.parse(cached);
    }

    // æ‰§è¡Œè½¬å½•
    const result = await this.whisperService.transcribe(audioUrl, options);
    
    // ç¼“å­˜ç»“æœ
    await this.cache.setex(cacheKey, 3600, JSON.stringify(result));
    
    return result;
  }

  async processText(text: string, promptType: PromptType, customPrompt?: string): Promise<string> {
    const prompt = this.buildPrompt(text, promptType, customPrompt);
    
    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = this.generateCacheKey('process', text, promptType, customPrompt);
    const cached = await this.cache.get(cacheKey);
    if (cached) {
      return cached;
    }

    // æ‰§è¡Œå¤„ç†
    const result = await this.llmService.generate(prompt);
    
    // ç¼“å­˜ç»“æœ
    await this.cache.setex(cacheKey, 1800, result);
    
    return result;
  }

  async processTextStream(text: string, promptType: PromptType, customPrompt?: string): AsyncIterableIterator<string> {
    const prompt = this.buildPrompt(text, promptType, customPrompt);
    return this.llmService.generateStream(prompt);
  }

  private buildPrompt(text: string, promptType: PromptType, customPrompt?: string): string {
    const template = promptType === 'custom' && customPrompt 
      ? customPrompt 
      : PROMPT_TEMPLATES[promptType];
    
    return template.replace('{transcription}', text).replace('{prompt}', customPrompt || '');
  }

  private generateCacheKey(...args: any[]): string {
    const hash = crypto.createHash('md5');
    hash.update(JSON.stringify(args));
    return `ai_cache:${hash.digest('hex')}`;
  }
}
```

## ğŸ“ˆ æ¨¡å‹ç®¡ç†

### æ¨¡å‹ä¸‹è½½å’Œæ›´æ–°

**æ¨¡å‹ç®¡ç†è„šæœ¬**:
```bash
#!/bin/bash
# scripts/management/ai-setup.sh

OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-"http://localhost:11434"}

# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
check_ollama_status() {
    curl -f "$OLLAMA_BASE_URL/api/tags" >/dev/null 2>&1
    return $?
}

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
list_models() {
    echo "æœ¬åœ°å·²å®‰è£…çš„æ¨¡å‹ï¼š"
    curl -s "$OLLAMA_BASE_URL/api/tags" | jq -r '.models[].name'
}

# å®‰è£…æ¨èæ¨¡å‹
install_recommended_models() {
    echo "æ­£åœ¨å®‰è£…æ¨èçš„AIæ¨¡å‹..."
    
    # Whisperæ¨¡å‹
    echo "ä¸‹è½½Whisperæ¨¡å‹..."
    docker exec whisper_ollama ollama pull whisper:latest
    
    # ä¸­æ–‡ä¼˜åŒ–çš„LLMæ¨¡å‹
    echo "ä¸‹è½½ä¸­æ–‡LLMæ¨¡å‹..."
    docker exec whisper_ollama ollama pull qwen2:7b
    
    # é€šç”¨LLMæ¨¡å‹
    echo "ä¸‹è½½é€šç”¨LLMæ¨¡å‹..."
    docker exec whisper_ollama ollama pull llama3.1:8b
    
    echo "æ¨¡å‹å®‰è£…å®Œæˆï¼"
}

# åˆ é™¤æ¨¡å‹
remove_model() {
    local model_name=$1
    if [ -z "$model_name" ]; then
        echo "è¯·æŒ‡å®šè¦åˆ é™¤çš„æ¨¡å‹åç§°"
        return 1
    fi
    
    docker exec whisper_ollama ollama rm "$model_name"
    echo "æ¨¡å‹ $model_name å·²åˆ é™¤"
}

# äº¤äº’å¼è®¾ç½®
interactive_setup() {
    echo "=== Whisper App AIæ¨¡å‹è®¾ç½® ==="
    echo "1. å®‰è£…æ¨èæ¨¡å‹"
    echo "2. åˆ—å‡ºå·²å®‰è£…æ¨¡å‹"
    echo "3. åˆ é™¤æ¨¡å‹"
    echo "4. æ£€æŸ¥æœåŠ¡çŠ¶æ€"
    echo "5. é€€å‡º"
    
    read -p "è¯·é€‰æ‹©æ“ä½œ (1-5): " choice
    
    case $choice in
        1) install_recommended_models ;;
        2) list_models ;;
        3) 
            read -p "è¯·è¾“å…¥è¦åˆ é™¤çš„æ¨¡å‹åç§°: " model_name
            remove_model "$model_name"
            ;;
        4) 
            if check_ollama_status; then
                echo "âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸"
            else
                echo "âŒ OllamaæœåŠ¡ä¸å¯ç”¨"
            fi
            ;;
        5) exit 0 ;;
        *) echo "æ— æ•ˆé€‰æ‹©" ;;
    esac
}

# ä¸»ç¨‹åº
main() {
    if [ "$#" -eq 0 ]; then
        interactive_setup
    else
        case "$1" in
            "list") list_models ;;
            "install") install_recommended_models ;;
            "remove") remove_model "$2" ;;
            "status") check_ollama_status ;;
            *) echo "ç”¨æ³•: $0 [list|install|remove|status]" ;;
        esac
    fi
}

main "$@"
```

### æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```typescript
// lib/modelManager.ts
export class ModelManager {
  private ollamaUrl: string;

  constructor() {
    this.ollamaUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
  }

  async getInstalledModels(): Promise<ModelInfo[]> {
    const response = await fetch(`${this.ollamaUrl}/api/tags`);
    const data = await response.json();
    return data.models.map((model: any) => ({
      name: model.name,
      size: model.size,
      modified: new Date(model.modified_at)
    }));
  }

  async downloadModel(modelName: string): Promise<void> {
    const response = await fetch(`${this.ollamaUrl}/api/pull`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ name: modelName })
    });

    if (!response.ok) {
      throw new Error(`Failed to download model: ${modelName}`);
    }

    // ç›‘å¬ä¸‹è½½è¿›åº¦
    const reader = response.body?.getReader();
    if (reader) {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = new TextDecoder().decode(value);
        const progress = JSON.parse(chunk);
        console.log(`ä¸‹è½½è¿›åº¦: ${progress.completed || 0}/${progress.total || 0}`);
      }
    }
  }

  async removeModel(modelName: string): Promise<void> {
    const response = await fetch(`${this.ollamaUrl}/api/delete`, {
      method: 'DELETE',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ name: modelName })
    });

    if (!response.ok) {
      throw new Error(`Failed to remove model: ${modelName}`);
    }
  }
}
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### GPUåŠ é€Ÿé…ç½®

**NVIDIA GPUæ”¯æŒ**:
```yaml
# docker-compose.yml
ollama:
  image: ollama/ollama:latest
  runtime: nvidia  # å¯ç”¨GPUæ”¯æŒ
  environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - OLLAMA_GPU=1
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
```

**GPUæ£€æŸ¥è„šæœ¬**:
```bash
#!/bin/bash
# æ£€æŸ¥GPUå¯ç”¨æ€§
nvidia-smi

# åœ¨å®¹å™¨å†…æ£€æŸ¥GPU
docker exec whisper_ollama nvidia-smi

# æµ‹è¯•GPUåŠ é€Ÿ
docker exec whisper_ollama ollama run llama3.1:8b "Hello" --verbose
```

### å†…å­˜ä¼˜åŒ–

**æ¨¡å‹é‡åŒ–**:
```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜ä½¿ç”¨
docker exec whisper_ollama ollama pull llama3.1:8b-q4_0  # 4-bité‡åŒ–
docker exec whisper_ollama ollama pull llama3.1:8b-q8_0  # 8-bité‡åŒ–
```

**å†…å­˜ç®¡ç†é…ç½®**:
```typescript
// é…ç½®æ¨¡å‹å†…å­˜é™åˆ¶
const generateOptions: GenerateOptions = {
  model: 'llama3.1:8b',
  options: {
    num_ctx: 2048,        // å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
    num_batch: 512,       // å‡å°‘æ‰¹å¤„ç†å¤§å°
    num_gqa: 1,          // å‡å°‘æ³¨æ„åŠ›å¤´æ•°
    num_gpu: 1,          // æŒ‡å®šGPUå±‚æ•°
    num_thread: 4,       // é™åˆ¶CPUçº¿ç¨‹æ•°
  }
};
```

### ç¼“å­˜ç­–ç•¥

**å¤šçº§ç¼“å­˜æ¶æ„**:
```typescript
export class CacheManager {
  private memoryCache: Map<string, any> = new Map();
  private redisCache: Redis;

  constructor() {
    this.redisCache = new Redis(process.env.REDIS_URL);
  }

  async get(key: string): Promise<any> {
    // 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    if (this.memoryCache.has(key)) {
      return this.memoryCache.get(key);
    }

    // 2. æ£€æŸ¥Redisç¼“å­˜
    const cached = await this.redisCache.get(key);
    if (cached) {
      const value = JSON.parse(cached);
      this.memoryCache.set(key, value);
      return value;
    }

    return null;
  }

  async set(key: string, value: any, ttl: number = 3600): Promise<void> {
    // è®¾ç½®å†…å­˜ç¼“å­˜
    this.memoryCache.set(key, value);
    
    // è®¾ç½®Redisç¼“å­˜
    await this.redisCache.setex(key, ttl, JSON.stringify(value));
  }
}
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œå®¹é”™

### æœåŠ¡å¯ç”¨æ€§æ£€æŸ¥

```typescript
export class ServiceHealthChecker {
  async checkWhisperService(): Promise<boolean> {
    try {
      const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/tags`, {
        timeout: 5000
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  async checkLLMService(): Promise<boolean> {
    try {
      const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'llama3.1:8b',
          prompt: 'test',
          stream: false
        }),
        timeout: 10000
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  async getSystemStatus(): Promise<SystemStatus> {
    const [whisperOk, llmOk] = await Promise.all([
      this.checkWhisperService(),
      this.checkLLMService()
    ]);

    return {
      whisper: whisperOk ? 'healthy' : 'unhealthy',
      llm: llmOk ? 'healthy' : 'unhealthy',
      overall: (whisperOk && llmOk) ? 'healthy' : 'degraded'
    };
  }
}
```

### é‡è¯•æœºåˆ¶

```typescript
export class RetryManager {
  async withRetry<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3,
    delay: number = 1000
  ): Promise<T> {
    let lastError: Error;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error as Error;
        
        if (attempt === maxRetries) {
          throw lastError;
        }

        // æŒ‡æ•°é€€é¿
        const waitTime = delay * Math.pow(2, attempt - 1);
        await new Promise(resolve => setTimeout(resolve, waitTime));
      }
    }

    throw lastError!;
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const retryManager = new RetryManager();
const result = await retryManager.withRetry(
  () => whisperService.transcribe(audioUrl),
  3,
  2000
);
```

### é™çº§ç­–ç•¥

```typescript
export class FallbackManager {
  private primaryService: LocalWhisperService;
  private fallbackService?: ExternalWhisperService;

  async transcribeWithFallback(audioUrl: string): Promise<TranscribeResult> {
    try {
      // å°è¯•æœ¬åœ°æœåŠ¡
      return await this.primaryService.transcribe(audioUrl);
    } catch (error) {
      console.warn('æœ¬åœ°WhisperæœåŠ¡å¤±è´¥ï¼Œå°è¯•é™çº§æ–¹æ¡ˆ', error);

      if (this.fallbackService) {
        try {
          return await this.fallbackService.transcribe(audioUrl);
        } catch (fallbackError) {
          console.error('é™çº§æœåŠ¡ä¹Ÿå¤±è´¥äº†', fallbackError);
        }
      }

      // æœ€åçš„é™çº§ï¼šè¿”å›é”™è¯¯ä¿¡æ¯
      return {
        text: 'è½¬å½•æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•',
        language: 'unknown',
        confidence: 0
      };
    }
  }
}
```

## ğŸ“Š ç›‘æ§å’Œè¯Šæ–­

### æ€§èƒ½ç›‘æ§

```typescript
export class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();

  recordLatency(operation: string, latency: number): void {
    if (!this.metrics.has(operation)) {
      this.metrics.set(operation, []);
    }
    this.metrics.get(operation)!.push(latency);
  }

  getAverageLatency(operation: string): number {
    const latencies = this.metrics.get(operation) || [];
    return latencies.reduce((sum, lat) => sum + lat, 0) / latencies.length;
  }

  async measureOperation<T>(operation: string, fn: () => Promise<T>): Promise<T> {
    const start = Date.now();
    try {
      const result = await fn();
      const latency = Date.now() - start;
      this.recordLatency(operation, latency);
      return result;
    } catch (error) {
      const latency = Date.now() - start;
      this.recordLatency(`${operation}_error`, latency);
      throw error;
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const monitor = new PerformanceMonitor();

const result = await monitor.measureOperation('whisper_transcribe', async () => {
  return await whisperService.transcribe(audioUrl);
});
```

### è¯Šæ–­å·¥å…·

```bash
#!/bin/bash
# scripts/ai-diagnostics.sh

echo "=== AIæœåŠ¡è¯Šæ–­æŠ¥å‘Š ==="

# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
echo "1. OllamaæœåŠ¡çŠ¶æ€ï¼š"
curl -s http://localhost:11434/api/tags | jq -r '.models[] | "\(.name) - \(.size) bytes"'

# æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
echo "2. GPUçŠ¶æ€ï¼š"
nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits

# æ£€æŸ¥å®¹å™¨èµ„æºä½¿ç”¨
echo "3. å®¹å™¨èµ„æºä½¿ç”¨ï¼š"
docker stats whisper_ollama --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

# æµ‹è¯•Whisperæ¨¡å‹
echo "4. Whisperæ¨¡å‹æµ‹è¯•ï¼š"
echo "Testing..." | docker exec -i whisper_ollama ollama run whisper:latest

# æµ‹è¯•LLMæ¨¡å‹
echo "5. LLMæ¨¡å‹æµ‹è¯•ï¼š"
echo "Hello" | docker exec -i whisper_ollama ollama run llama3.1:8b
```

## ğŸ”® æ‰©å±•å’Œå®šåˆ¶

### è‡ªå®šä¹‰æ¨¡å‹é›†æˆ

```typescript
// lib/customModelAdapter.ts
export interface ModelAdapter {
  name: string;
  type: 'whisper' | 'llm';
  transcribe?(audioUrl: string, options?: any): Promise<TranscribeResult>;
  generate?(prompt: string, options?: any): Promise<string>;
}

export class HuggingFaceAdapter implements ModelAdapter {
  name = 'huggingface';
  type = 'whisper' as const;

  async transcribe(audioUrl: string, options?: any): Promise<TranscribeResult> {
    // é›†æˆHuggingFace Whisperæ¨¡å‹
    const response = await fetch('https://api-inference.huggingface.co/models/openai/whisper-large-v3', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ inputs: audioUrl })
    });

    const result = await response.json();
    return {
      text: result.text,
      language: result.language || 'auto',
      confidence: result.confidence || 0.95
    };
  }
}
```

### æ’ä»¶ç³»ç»Ÿæ¶æ„

```typescript
export interface AIPlugin {
  name: string;
  version: string;
  description: string;
  install(): Promise<void>;
  uninstall(): Promise<void>;
  process(input: any, context: PluginContext): Promise<any>;
}

export class PluginManager {
  private plugins: Map<string, AIPlugin> = new Map();

  async loadPlugin(plugin: AIPlugin): Promise<void> {
    await plugin.install();
    this.plugins.set(plugin.name, plugin);
  }

  async executePlugin(name: string, input: any, context: PluginContext): Promise<any> {
    const plugin = this.plugins.get(name);
    if (!plugin) {
      throw new Error(`Plugin not found: ${name}`);
    }
    return await plugin.process(input, context);
  }
}
```

### æœªæ¥æ‰©å±•è®¡åˆ’

1. **å¤šæ¨¡æ€æ”¯æŒ**: å›¾åƒå’Œè§†é¢‘å†…å®¹ç†è§£
2. **å®æ—¶è½¬å½•**: WebSocketå®æ—¶éŸ³é¢‘æµå¤„ç†
3. **è¯­éŸ³åˆæˆ**: TTSåŠŸèƒ½é›†æˆ
4. **æƒ…æ„Ÿåˆ†æ**: è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«
5. **è¯´è¯äººè¯†åˆ«**: å¤šè¯´è¯äººåˆ†ç¦»å’Œè¯†åˆ«

---

**è¿™ä»½AIæœåŠ¡é›†æˆæ–‡æ¡£ä¸ºç³»ç»Ÿçš„æ™ºèƒ½åŠŸèƒ½æä¾›äº†å…¨é¢çš„æŠ€æœ¯è¯´æ˜ã€‚** ğŸ¤–

å¦‚éœ€äº†è§£æ›´å¤šå®ç°ç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹ [DockeræœåŠ¡é…ç½®æ–‡æ¡£](./DOCKER_SERVICES.md) æˆ– [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](../user/BEST_PRACTICES.md)ã€‚