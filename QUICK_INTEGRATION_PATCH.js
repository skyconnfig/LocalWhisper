/**
 * æœ¬åœ°LLMæœåŠ¡å¿«é€Ÿé›†æˆè¡¥ä¸
 * 
 * å°†æ­¤ä»£ç æ›¿æ¢app/api/transform/route.tsä¸­çš„ç¬¬75-79è¡Œ:
 * 
 * åŸä»£ç :
 * const aiClient = togetherVercelAiClient(apiKey);
 * const { textStream } = streamText({
 *   model: aiClient("meta-llama/Meta-Llama-3-70B-Instruct-Turbo"),
 *   prompt,
 * });
 */

// 1. é¦–å…ˆåœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ import (åœ¨ç°æœ‰importsåé¢)
import { createUnifiedLLMClient, checkOllamaHealth } from "@/lib/apiClients";

// 2. å°†ç¬¬75-79è¡Œæ›¿æ¢ä¸ºä»¥ä¸‹ä»£ç :

  // æ£€æŸ¥æ˜¯å¦å¯ç”¨æœ¬åœ°LLM
  const useLocalLLM = process.env.USE_LOCAL_LLM === "true" || 
                     req.headers.get("X-Use-Local-LLM") === "true";
  let aiClient;
  let model;
  
  if (useLocalLLM) {
    try {
      // æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€
      const healthCheck = await checkOllamaHealth();
      if (healthCheck.status === 'healthy') {
        console.log("âœ… ä½¿ç”¨æœ¬åœ°LLMæœåŠ¡ (Ollama)");
        aiClient = createUnifiedLLMClient(true);
        // æ™ºèƒ½é€‰æ‹©æ¨¡å‹ï¼šæ ¹æ®è½¬å½•å†…å®¹è¯­è¨€è‡ªåŠ¨é€‰æ‹©ä¸­æ–‡æˆ–è‹±æ–‡æ¨¡å‹
        model = aiClient(whisper.fullTranscription);
      } else {
        console.warn("âš ï¸ æœ¬åœ°LLMä¸å¯ç”¨ï¼Œå›é€€åˆ°Together.ai:", healthCheck.error);
        aiClient = togetherVercelAiClient(apiKey); 
        model = aiClient("meta-llama/Meta-Llama-3-70B-Instruct-Turbo");
      }
    } catch (error) {
      console.error("âŒ æœ¬åœ°LLMæ£€æŸ¥å¤±è´¥ï¼Œå›é€€åˆ°Together.ai:", error);
      aiClient = togetherVercelAiClient(apiKey);
      model = aiClient("meta-llama/Meta-Llama-3-70B-Instruct-Turbo");
    }
  } else {
    console.log("ğŸŒ ä½¿ç”¨Together.aiæœåŠ¡");
    aiClient = togetherVercelAiClient(apiKey);
    model = aiClient("meta-llama/Meta-Llama-3-70B-Instruct-Turbo");
  }

  // å¼€å§‹æµå¼å¤„ç† (ä¿æŒåŸæœ‰çš„streamTextè°ƒç”¨)
  const { textStream } = await streamText({
    model,
    prompt,
  });

// 3. åœ¨return Responseçš„headersä¸­æ·»åŠ æœåŠ¡æ ‡è¯† (å¯é€‰):
  return new Response(stream, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "X-Accel-Buffering": "no", 
      "Cache-Control": "no-cache",
      "X-LLM-Service": useLocalLLM ? "local-ollama" : "together-ai", // æ·»åŠ è¿™è¡Œ
    },
  });

/**
 * ä½¿ç”¨æ­¥éª¤:
 * 
 * 1. ç¡®ä¿å·²åˆ›å»º /lib/localLLMService.ts æ–‡ä»¶
 * 2. æ›´æ–° /lib/apiClients.ts æ–‡ä»¶ (æ·»åŠ æœ¬åœ°LLMå¯¼å‡º)
 * 3. åœ¨ .env.local ä¸­è®¾ç½®: USE_LOCAL_LLM=true
 * 4. å®‰è£…å¹¶è¿è¡ŒOllama: 
 *    - ollama pull qwen2.5:7b
 *    - ollama pull llama3.1:8b  
 *    - ollama serve
 * 5. é‡å¯åº”ç”¨: npm run dev
 * 
 * åŠŸèƒ½:
 * - è‡ªåŠ¨è¯­è¨€æ£€æµ‹ (ä¸­æ–‡ç”¨qwen2.5:7b, è‹±æ–‡ç”¨llama3.1:8b)
 * - å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨å›é€€
 * - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
 * - ä¸ç°æœ‰APIå®Œå…¨å…¼å®¹
 */