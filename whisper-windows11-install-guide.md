# Whisper Windows 11 å®Œæ•´å®‰è£…æ‰‹å†Œ

> OpenAI Whisper è¯­éŸ³è½¬å½•å·¥å…·åœ¨ Windows 11 ç³»ç»Ÿä¸Šçš„è¯¦ç»†å®‰è£…é…ç½®æŒ‡å—ï¼ŒåŒ…å«å®˜æ–¹ç‰ˆæœ¬ã€ä¼˜åŒ–ç‰ˆæœ¬å’ŒGPUåŠ é€Ÿé…ç½®

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [Pythonç¯å¢ƒé…ç½®](#pythonç¯å¢ƒé…ç½®)
- [OpenAI Whisperå®‰è£…](#openai-whisperå®‰è£…)
- [Faster-Whisperå®‰è£…](#faster-whisperå®‰è£…æ¨è)
- [æ¨¡å‹ä¸‹è½½ç®¡ç†](#æ¨¡å‹ä¸‹è½½ç®¡ç†)
- [GPUåŠ é€Ÿé…ç½®](#gpuåŠ é€Ÿé…ç½®)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **æ“ä½œç³»ç»Ÿ**ï¼šWindows 11 (64ä½)
- **å†…å­˜**ï¼š8GB RAMï¼ˆæ¨è16GB+ï¼‰
- **å­˜å‚¨ç©ºé—´**ï¼š5GBå¯ç”¨ç©ºé—´ï¼ˆç”¨äºPythonç¯å¢ƒå’Œæ¨¡å‹ï¼‰
- **å¤„ç†å™¨**ï¼šIntel i5 / AMD Ryzen 5 æˆ–æ›´é«˜

### GPUåŠ é€Ÿé…ç½®ï¼ˆå¯é€‰ï¼‰
- **æ˜¾å¡**ï¼šNVIDIA RTX 20ç³»åˆ—æˆ–æ›´æ–°ï¼ˆæ”¯æŒCUDAï¼‰
- **æ˜¾å­˜**ï¼š4GB VRAMï¼ˆåŸºç¡€æ¨¡å‹ï¼‰/ 8GB+ï¼ˆå¤§å‹æ¨¡å‹ï¼‰
- **CUDAç‰ˆæœ¬**ï¼š11.7æˆ–æ›´é«˜

### æ¨èé…ç½®
- **å†…å­˜**ï¼š32GB RAM
- **å­˜å‚¨**ï¼šSSDç¡¬ç›˜ï¼Œ20GB+å¯ç”¨ç©ºé—´
- **æ˜¾å¡**ï¼šNVIDIA RTX 30/40ç³»åˆ—ï¼Œ8GB+ VRAM
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆé¦–æ¬¡ä¸‹è½½æ¨¡å‹ï¼‰

---

## ğŸ Pythonç¯å¢ƒé…ç½®

Whisperéœ€è¦Python 3.8+ç¯å¢ƒï¼Œæ¨èä½¿ç”¨Python 3.10æˆ–3.11ã€‚

### 1. å®‰è£…Python

#### æ–¹æ³•ä¸€ï¼šå®˜æ–¹å®‰è£…åŒ…ï¼ˆæ¨èï¼‰
1. è®¿é—® [Pythonå®˜ç½‘](https://www.python.org/downloads/)
2. ä¸‹è½½æœ€æ–°çš„Python 3.11.xç‰ˆæœ¬
3. è¿è¡Œå®‰è£…ç¨‹åºï¼Œ**é‡è¦é€‰é¡¹**ï¼š
   - âœ… **Add Python to PATH**
   - âœ… **Install for all users**
   - é€‰æ‹© **Customize installation**
   - âœ… **pip**
   - âœ… **tcl/tk and IDLE**
   - âœ… **Python test suite**
   - âœ… **py launcher**
   - âœ… **for all users**

#### æ–¹æ³•äºŒï¼šä½¿ç”¨Chocolateyï¼ˆé«˜çº§ç”¨æˆ·ï¼‰
```powershell
# ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡ŒPowerShell
Set-ExecutionPolicy AllSigned
iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))

# å®‰è£…Python
choco install python311 -y
```

### 2. éªŒè¯Pythonå®‰è£…

```powershell
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version
# è¾“å‡ºåº”ç±»ä¼¼ï¼šPython 3.11.x

# æ£€æŸ¥pipç‰ˆæœ¬
pip --version
# è¾“å‡ºåº”ç±»ä¼¼ï¼špip 23.x.x from ...
```

### 3. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

```powershell
# åˆ›å»ºä¸“ç”¨ç›®å½•
mkdir C:\whisper
cd C:\whisper

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv whisper-env

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
.\whisper-env\Scripts\activate

# å‡çº§pip
python -m pip install --upgrade pip
```

---

## ğŸ¤ OpenAI Whisperå®‰è£…

OpenAIå®˜æ–¹Whisperæ˜¯æœ€æ ‡å‡†çš„å®ç°ï¼ŒåŠŸèƒ½å®Œæ•´ä½†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚

### 1. å®‰è£…ä¾èµ–

```powershell
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
.\whisper-env\Scripts\activate

# å®‰è£…å¿…è¦ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…FFmpegä¾èµ–ï¼ˆéŸ³é¢‘å¤„ç†ï¼‰
pip install ffmpeg-python
```

### 2. å®‰è£…Whisper

```powershell
# å®‰è£…OpenAI Whisper
pip install openai-whisper

# éªŒè¯å®‰è£…
whisper --help
```

### 3. åŸºç¡€ä½¿ç”¨æµ‹è¯•

```powershell
# ä¸‹è½½æµ‹è¯•éŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
# æˆ–ä½¿ç”¨ä½ è‡ªå·±çš„éŸ³é¢‘æ–‡ä»¶

# è½¬å½•éŸ³é¢‘æ–‡ä»¶
whisper audio.mp3 --model base

# æŒ‡å®šè¾“å‡ºæ ¼å¼
whisper audio.mp3 --model base --output_format txt

# æŒ‡å®šè¯­è¨€ï¼ˆæé«˜å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼‰
whisper audio.mp3 --model base --language Chinese
```

---

## âš¡ Faster-Whisperå®‰è£…ï¼ˆæ¨èï¼‰

Faster-Whisperæ˜¯Whisperçš„ä¼˜åŒ–å®ç°ï¼Œé€Ÿåº¦æå‡4-5å€ï¼Œå†…å­˜å ç”¨å‡å°‘50%ã€‚

### 1. å®‰è£…Faster-Whisper

```powershell
# åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…
pip install faster-whisper

# å¦‚æœéœ€è¦GPUæ”¯æŒ
pip install faster-whisper[gpu]
```

### 2. Pythonä½¿ç”¨ç¤ºä¾‹

åˆ›å»º `test_faster_whisper.py` æ–‡ä»¶ï¼š

```python
from faster_whisper import WhisperModel
import time

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆCPUç‰ˆæœ¬ï¼‰
model = WhisperModel("base", device="cpu", compute_type="int8")

# å¦‚æœæœ‰GPUï¼ˆå¯é€‰ï¼‰
# model = WhisperModel("base", device="cuda", compute_type="float16")

def transcribe_audio(audio_path, language=None):
    """è½¬å½•éŸ³é¢‘æ–‡ä»¶"""
    print(f"å¼€å§‹è½¬å½•: {audio_path}")
    start_time = time.time()
    
    # è½¬å½•éŸ³é¢‘
    segments, info = model.transcribe(
        audio_path, 
        language=language,
        beam_size=5,
        word_timestamps=True,
        vad_filter=True  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    )
    
    # æ”¶é›†ç»“æœ
    transcription = ""
    for segment in segments:
        print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
        transcription += segment.text + " "
    
    end_time = time.time()
    print(f"è½¬å½•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"æ£€æµ‹è¯­è¨€: {info.language} (ç½®ä¿¡åº¦: {info.language_probability:.2f})")
    
    return transcription.strip()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    audio_file = "path/to/your/audio.mp3"
    
    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    result = transcribe_audio(audio_file)
    
    # æˆ–æŒ‡å®šä¸­æ–‡
    # result = transcribe_audio(audio_file, language="zh")
    
    print("\nå®Œæ•´è½¬å½•ç»“æœ:")
    print(result)
```

### 3. è¿è¡Œæµ‹è¯•

```powershell
python test_faster_whisper.py
```

---

## ğŸ“¥ æ¨¡å‹ä¸‹è½½ç®¡ç†

Whisperæä¾›å¤šç§å¤§å°çš„æ¨¡å‹ï¼Œæ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

### æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹åç§° | å‚æ•°é‡ | å¤§å° | é€Ÿåº¦ | å‡†ç¡®æ€§ | æ¨èç”¨é€” |
|---------|--------|------|------|--------|----------|
| **tiny** | 39M | ~39MB | â­â­â­â­â­ | â­â­ | å¿«é€Ÿæµ‹è¯• |
| **base** | 74M | ~142MB | â­â­â­â­ | â­â­â­ | æ—¥å¸¸ä½¿ç”¨ |
| **small** | 244M | ~466MB | â­â­â­ | â­â­â­â­ | å¹³è¡¡é€‰æ‹© |
| **medium** | 769M | ~1.5GB | â­â­ | â­â­â­â­ | **æ¨è** |
| **large** | 1550M | ~3GB | â­ | â­â­â­â­â­ | æœ€é«˜è´¨é‡ |
| **large-v2** | 1550M | ~3GB | â­ | â­â­â­â­â­ | æ”¹è¿›ç‰ˆæœ¬ |
| **large-v3** | 1550M | ~3GB | â­ | â­â­â­â­â­ | **æœ€æ–°æœ€ä½³** |

### æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹

```powershell
# æ–¹æ³•ä¸€ï¼šä½¿ç”¨whisperå‘½ä»¤é¢„ä¸‹è½½
whisper --model tiny --help    # ä¸‹è½½tinyæ¨¡å‹
whisper --model base --help    # ä¸‹è½½baseæ¨¡å‹
whisper --model medium --help  # ä¸‹è½½mediumæ¨¡å‹

# æ–¹æ³•äºŒï¼šPythonè„šæœ¬æ‰¹é‡ä¸‹è½½
```

åˆ›å»º `download_models.py`ï¼š
```python
from faster_whisper import WhisperModel
import os

def download_models():
    """æ‰¹é‡ä¸‹è½½Whisperæ¨¡å‹"""
    models = ["tiny", "base", "small", "medium", "large-v3"]
    
    for model_name in models:
        print(f"ä¸‹è½½æ¨¡å‹: {model_name}")
        try:
            model = WhisperModel(model_name, device="cpu")
            print(f"âœ… {model_name} ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}")
        print("-" * 50)

if __name__ == "__main__":
    download_models()
```

### æ¨¡å‹å­˜å‚¨ä½ç½®

```powershell
# Windowsæ¨¡å‹ç¼“å­˜ä½ç½®
echo $env:USERPROFILE\.cache\huggingface\hub

# æˆ–è€…
C:\Users\[ç”¨æˆ·å]\.cache\huggingface\hub
```

---

## ğŸš€ GPUåŠ é€Ÿé…ç½®

GPUåŠ é€Ÿå¯ä»¥æ˜¾è‘—æå‡è½¬å½•é€Ÿåº¦ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å¤§é‡éŸ³é¢‘æ–‡ä»¶ã€‚

### 1. æ£€æŸ¥GPUå…¼å®¹æ€§

```powershell
# æ£€æŸ¥NVIDIAæ˜¾å¡
nvidia-smi

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version
```

### 2. å®‰è£…CUDAå·¥å…·åŒ…

1. è®¿é—® [NVIDIA CUDAä¸‹è½½é¡µé¢](https://developer.nvidia.com/cuda-downloads)
2. é€‰æ‹©Windows -> x86_64 -> 11 -> exe(local)
3. ä¸‹è½½å¹¶å®‰è£…CUDA Toolkit 11.8æˆ–12.x
4. é‡å¯è®¡ç®—æœº

### 3. å®‰è£…GPUç‰ˆæœ¬ä¾èµ–

```powershell
# å¸è½½CPUç‰ˆæœ¬çš„PyTorch
pip uninstall torch torchvision torchaudio

# å®‰è£…GPUç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# éªŒè¯GPUæ”¯æŒ
python -c "import torch; print('CUDAå¯ç”¨:', torch.cuda.is_available())"
```

### 4. GPUä¼˜åŒ–é…ç½®

åˆ›å»º `gpu_whisper_test.py`ï¼š
```python
from faster_whisper import WhisperModel
import torch

def setup_gpu_model():
    """é…ç½®GPUåŠ é€Ÿçš„Whisperæ¨¡å‹"""
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return WhisperModel("base", device="cpu", compute_type="int8")
    
    print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # é…ç½®GPUæ¨¡å‹
    model = WhisperModel(
        "medium",  # å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
        device="cuda",
        compute_type="float16",  # ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
        device_index=0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    )
    
    return model

def benchmark_performance(model, audio_path):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time
    
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    start_time = time.time()
    
    segments, info = model.transcribe(audio_path, beam_size=5)
    
    # å¤„ç†ç»“æœ
    text = ""
    for segment in segments:
        text += segment.text + " "
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"è½¬å½•å®Œæˆ:")
    print(f"â±ï¸  è€—æ—¶: {duration:.2f}ç§’")
    print(f"ğŸ¯ è¯­è¨€: {info.language}")
    print(f"ğŸ“Š ç½®ä¿¡åº¦: {info.language_probability:.2f}")
    
    return text.strip()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = setup_gpu_model()
    
    # æ›¿æ¢ä¸ºå®é™…éŸ³é¢‘æ–‡ä»¶
    audio_file = "path/to/your/audio.mp3"
    result = benchmark_performance(model, audio_file)
    
    print(f"\nè½¬å½•ç»“æœ:\n{result}")
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### 1. å‘½ä»¤è¡ŒåŸºç¡€ä½¿ç”¨

```powershell
# åŸºæœ¬è½¬å½•
whisper audio.mp3

# æŒ‡å®šæ¨¡å‹
whisper audio.mp3 --model medium

# æŒ‡å®šè¯­è¨€ï¼ˆä¸­æ–‡ï¼‰
whisper audio.mp3 --model medium --language Chinese

# æŒ‡å®šè¾“å‡ºæ ¼å¼
whisper audio.mp3 --model medium --output_format srt --output_format txt

# æ‰¹é‡å¤„ç†
whisper *.mp3 --model medium --language Chinese

# å®æ—¶è½¬å½•ï¼ˆä»éº¦å…‹é£ï¼‰
whisper --model base --language Chinese --device_index 0
```

### 2. Pythoné›†æˆç¤ºä¾‹

åˆ›å»º `whisper_integration.py`ï¼š
```python
import os
import json
from faster_whisper import WhisperModel
from pathlib import Path

class WhisperTranscriber:
    def __init__(self, model_size="medium", device="cpu"):
        """åˆå§‹åŒ–Whisperè½¬å½•å™¨"""
        self.model = WhisperModel(
            model_size, 
            device=device,
            compute_type="int8" if device == "cpu" else "float16"
        )
        
    def transcribe_file(self, audio_path, language=None, output_dir="./output"):
        """è½¬å½•å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        audio_path = Path(audio_path)
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        print(f"è½¬å½•æ–‡ä»¶: {audio_path.name}")
        
        # æ‰§è¡Œè½¬å½•
        segments, info = self.model.transcribe(
            str(audio_path),
            language=language,
            word_timestamps=True,
            vad_filter=True
        )
        
        # å‡†å¤‡è¾“å‡ºæ•°æ®
        result = {
            "file": str(audio_path),
            "language": info.language,
            "language_probability": info.language_probability,
            "duration": info.duration,
            "segments": []
        }
        
        full_text = ""
        
        # å¤„ç†åˆ†æ®µç»“æœ
        for segment in segments:
            segment_data = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text,
                "words": []
            }
            
            # å¤„ç†è¯çº§æ—¶é—´æˆ³
            if segment.words:
                for word in segment.words:
                    segment_data["words"].append({
                        "start": word.start,
                        "end": word.end,
                        "word": word.word,
                        "probability": word.probability
                    })
            
            result["segments"].append(segment_data)
            full_text += segment.text + " "
        
        result["full_text"] = full_text.strip()
        
        # ä¿å­˜ç»“æœ
        base_name = audio_path.stem
        
        # ä¿å­˜JSONè¯¦ç»†ç»“æœ
        json_path = output_dir / f"{base_name}_transcription.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜çº¯æ–‡æœ¬
        txt_path = output_dir / f"{base_name}_transcription.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(result["full_text"])
        
        # ä¿å­˜SRTå­—å¹•æ–‡ä»¶
        srt_path = output_dir / f"{base_name}_subtitles.srt"
        self._save_srt(result["segments"], srt_path)
        
        print(f"âœ… è½¬å½•å®Œæˆ: {json_path}")
        return result
    
    def _save_srt(self, segments, srt_path):
        """ä¿å­˜SRTå­—å¹•æ ¼å¼"""
        with open(srt_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(segments, 1):
                start_time = self._format_timestamp(segment["start"])
                end_time = self._format_timestamp(segment["end"])
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text'].strip()}\n\n")
    
    def _format_timestamp(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºSRTæ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        milliseconds = int((seconds % 1) * 1000)
        seconds = int(seconds)
        
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"
    
    def batch_transcribe(self, input_dir, output_dir="./output", language=None):
        """æ‰¹é‡è½¬å½•ç›®å½•ä¸­çš„éŸ³é¢‘æ–‡ä»¶"""
        input_dir = Path(input_dir)
        audio_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.ogg', '.wma'}
        
        audio_files = [
            f for f in input_dir.rglob('*') 
            if f.suffix.lower() in audio_extensions
        ]
        
        print(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        results = []
        for audio_file in audio_files:
            try:
                result = self.transcribe_file(audio_file, language, output_dir)
                results.append(result)
            except Exception as e:
                print(f"âŒ å¤„ç† {audio_file} æ—¶å‡ºé”™: {e}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè½¬å½•å™¨
    transcriber = WhisperTranscriber(model_size="medium", device="cpu")
    
    # å•æ–‡ä»¶è½¬å½•
    # result = transcriber.transcribe_file("audio.mp3", language="zh")
    
    # æ‰¹é‡è½¬å½•
    # results = transcriber.batch_transcribe("./audio_files/", language="zh")
    
    print("è½¬å½•å™¨å·²å°±ç»ªï¼")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
def choose_optimal_model(audio_duration, quality_requirement="balanced"):
    """æ ¹æ®éŸ³é¢‘é•¿åº¦å’Œè´¨é‡è¦æ±‚é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
    
    if quality_requirement == "speed":
        if audio_duration < 300:  # 5åˆ†é’Ÿä»¥å†…
            return "base"
        else:
            return "small"
    
    elif quality_requirement == "balanced":
        if audio_duration < 600:  # 10åˆ†é’Ÿä»¥å†…
            return "medium"
        else:
            return "base"
    
    elif quality_requirement == "quality":
        return "large-v3"
    
    return "medium"  # é»˜è®¤é€‰æ‹©
```

### 2. å†…å­˜ä¼˜åŒ–é…ç½®

```python
import gc
import torch

def optimize_memory():
    """å†…å­˜ä¼˜åŒ–è®¾ç½®"""
    # æ¸…ç†ç¼“å­˜
    gc.collect()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.set_per_process_memory_fraction(0.8)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
def batch_process_optimization(audio_files, batch_size=4):
    """æ‰¹å¤„ç†ä¼˜åŒ–"""
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œé¿å…å†…å­˜å³°å€¼
    audio_files.sort(key=lambda x: os.path.getsize(x))
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(audio_files), batch_size):
        batch = audio_files[i:i + batch_size]
        
        # å¤„ç†æ‰¹æ¬¡
        for audio_file in batch:
            process_audio(audio_file)
        
        # æ¸…ç†å†…å­˜
        optimize_memory()
```

---

## ğŸ”§ é›†æˆåˆ°Webåº”ç”¨

### Node.jsé›†æˆç¤ºä¾‹

åˆ›å»º `whisper-node-integration.js`ï¼š
```javascript
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs').promises;

class WhisperService {
    constructor(modelSize = 'medium', pythonPath = 'python') {
        this.modelSize = modelSize;
        this.pythonPath = pythonPath;
        this.scriptPath = path.join(__dirname, 'whisper_service.py');
    }

    async transcribe(audioPath, language = null) {
        return new Promise((resolve, reject) => {
            const args = [this.scriptPath, audioPath, this.modelSize];
            if (language) args.push('--language', language);

            const process = spawn(this.pythonPath, args);
            
            let output = '';
            let error = '';

            process.stdout.on('data', (data) => {
                output += data.toString();
            });

            process.stderr.on('data', (data) => {
                error += data.toString();
            });

            process.on('close', (code) => {
                if (code === 0) {
                    try {
                        const result = JSON.parse(output);
                        resolve(result);
                    } catch (e) {
                        reject(new Error('Failed to parse Whisper output'));
                    }
                } else {
                    reject(new Error(`Whisper process failed: ${error}`));
                }
            });
        });
    }
}

// ä½¿ç”¨ç¤ºä¾‹
async function example() {
    const whisper = new WhisperService('medium');
    
    try {
        const result = await whisper.transcribe('./audio.mp3', 'zh');
        console.log('è½¬å½•ç»“æœ:', result.text);
    } catch (error) {
        console.error('è½¬å½•å¤±è´¥:', error);
    }
}

module.exports = WhisperService;
```

å¯¹åº”çš„PythonæœåŠ¡è„šæœ¬ `whisper_service.py`ï¼š
```python
#!/usr/bin/env python3
import sys
import json
import argparse
from faster_whisper import WhisperModel

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('audio_path', help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('model_size', help='æ¨¡å‹å¤§å°')
    parser.add_argument('--language', help='è¯­è¨€ä»£ç ')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = WhisperModel(args.model_size, device="cpu", compute_type="int8")
        
        # è½¬å½•éŸ³é¢‘
        segments, info = model.transcribe(
            args.audio_path,
            language=args.language,
            word_timestamps=True
        )
        
        # æ„å»ºç»“æœ
        result = {
            'success': True,
            'language': info.language,
            'language_probability': info.language_probability,
            'duration': info.duration,
            'text': '',
            'segments': []
        }
        
        for segment in segments:
            segment_data = {
                'start': segment.start,
                'end': segment.end,
                'text': segment.text
            }
            result['segments'].append(segment_data)
            result['text'] += segment.text + ' '
        
        result['text'] = result['text'].strip()
        
        # è¾“å‡ºJSONç»“æœ
        print(json.dumps(result, ensure_ascii=False))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e)
        }
        print(json.dumps(error_result, ensure_ascii=False))
        sys.exit(1)

if __name__ == '__main__':
    main()
```

---

## ğŸš¨ å¸¸è§é—®é¢˜

### å®‰è£…é—®é¢˜

**Q: å®‰è£…æ—¶æç¤º"Microsoft Visual C++ 14.0 is required"**
```powershell
# è§£å†³æ–¹æ¡ˆï¼šå®‰è£…Visual Studio Build Tools
# 1. ä¸‹è½½ Visual Studio Build Tools
# 2. æˆ–è€…å®‰è£…å®Œæ•´çš„Visual Studio Community
# 3. ç¡®ä¿åŒ…å«C++æ„å»ºå·¥å…·
```

**Q: pipå®‰è£…è¶…æ—¶æˆ–å¤±è´¥**
```powershell
# ä½¿ç”¨å›½å†…é•œåƒæº
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple openai-whisper
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple faster-whisper
```

### è¿è¡Œé—®é¢˜

**Q: æç¤ºæ‰¾ä¸åˆ°FFmpeg**
```powershell
# æ–¹æ³•ä¸€ï¼šä½¿ç”¨Chocolateyå®‰è£…
choco install ffmpeg

# æ–¹æ³•äºŒï¼šæ‰‹åŠ¨ä¸‹è½½
# 1. ä» https://ffmpeg.org/download.html ä¸‹è½½
# 2. è§£å‹åˆ°C:\ffmpeg
# 3. å°†C:\ffmpeg\binæ·»åŠ åˆ°PATHç¯å¢ƒå˜é‡
```

**Q: CUDAå†…å­˜ä¸è¶³**
```python
# å‡å°‘æ¨¡å‹å¤§å°æˆ–ä½¿ç”¨CPU
model = WhisperModel("base", device="cpu")

# æˆ–è€…ä½¿ç”¨æ··åˆç²¾åº¦
model = WhisperModel("medium", device="cuda", compute_type="int8")
```

**Q: è½¬å½•ç»“æœä¸å‡†ç¡®**
```python
# æ˜ç¡®æŒ‡å®šè¯­è¨€
segments, info = model.transcribe(audio_path, language="zh")

# æé«˜beam_size
segments, info = model.transcribe(audio_path, beam_size=5)

# å¯ç”¨VADæ»¤æ³¢
segments, info = model.transcribe(audio_path, vad_filter=True)
```

### æ€§èƒ½é—®é¢˜

**Q: è½¬å½•é€Ÿåº¦å¤ªæ…¢**
1. ä½¿ç”¨Faster-Whisperè€ŒéOpenAI Whisper
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆbaseä»£æ›¿mediumï¼‰
3. å¯ç”¨GPUåŠ é€Ÿ
4. æ˜ç¡®æŒ‡å®šéŸ³é¢‘è¯­è¨€

**Q: å†…å­˜å ç”¨è¿‡é«˜**
```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
model = WhisperModel("medium", compute_type="int8")

# å®šæœŸæ¸…ç†å†…å­˜
import gc
gc.collect()
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### åˆ›å»ºåŸºå‡†æµ‹è¯•è„šæœ¬

`benchmark_whisper.py`:
```python
import time
import psutil
import torch
from faster_whisper import WhisperModel

def benchmark_models(audio_path, models=["base", "medium", "large-v3"]):
    """åŸºå‡†æµ‹è¯•ä¸åŒæ¨¡å‹æ€§èƒ½"""
    
    results = []
    
    for model_name in models:
        print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
        print("-" * 50)
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_memory = psutil.virtual_memory().used / 1024**3  # GB
        start_time = time.time()
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = WhisperModel(model_name, device="cpu", compute_type="int8")
        
        # æ‰§è¡Œè½¬å½•
        segments, info = model.transcribe(audio_path, beam_size=5)
        
        # å¤„ç†ç»“æœ
        text_length = 0
        segment_count = 0
        for segment in segments:
            text_length += len(segment.text)
            segment_count += 1
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024**3  # GB
        
        # è®¡ç®—æŒ‡æ ‡
        duration = end_time - start_time
        memory_used = end_memory - start_memory
        
        result = {
            'model': model_name,
            'duration': duration,
            'memory_used': memory_used,
            'text_length': text_length,
            'segment_count': segment_count,
            'language': info.language,
            'confidence': info.language_probability,
            'speed_factor': info.duration / duration if info.duration > 0 else 0
        }
        
        results.append(result)
        
        print(f"â±ï¸  è½¬å½•æ—¶é—´: {duration:.2f}ç§’")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_used:.2f}GB")
        print(f"ğŸ“Š æ–‡æœ¬é•¿åº¦: {text_length}å­—ç¬¦")
        print(f"ğŸ¯ æ£€æµ‹è¯­è¨€: {info.language} ({info.language_probability:.2f})")
        print(f"ğŸš€ é€Ÿåº¦å€æ•°: {result['speed_factor']:.2f}x")
        
        # æ¸…ç†å†…å­˜
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return results

def print_comparison(results):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    print("="*80)
    
    print(f"{'æ¨¡å‹':<12} {'æ—¶é—´(ç§’)':<10} {'å†…å­˜(GB)':<10} {'é€Ÿåº¦å€æ•°':<10} {'å‡†ç¡®åº¦':<8}")
    print("-" * 60)
    
    for result in results:
        print(f"{result['model']:<12} {result['duration']:<10.2f} "
              f"{result['memory_used']:<10.2f} {result['speed_factor']:<10.2f} "
              f"{result['confidence']:<8.2f}")

# è¿è¡ŒåŸºå‡†æµ‹è¯•
if __name__ == "__main__":
    audio_file = "path/to/your/test_audio.mp3"  # æ›¿æ¢ä¸ºå®é™…éŸ³é¢‘æ–‡ä»¶
    
    if os.path.exists(audio_file):
        results = benchmark_models(audio_file)
        print_comparison(results)
    else:
        print("è¯·æä¾›æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„è¿›è¡ŒåŸºå‡†æµ‹è¯•")
```

---

## ğŸ¯ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®

### 1. æœåŠ¡åŒ–éƒ¨ç½²

åˆ›å»ºWindowsæœåŠ¡é…ç½® `whisper-service.py`:
```python
import servicemanager
import win32serviceutil
import win32service
import win32event
import socket
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
from faster_whisper import WhisperModel

class WhisperService(win32serviceutil.ServiceFramework):
    _svc_name_ = "WhisperTranscriptionService"
    _svc_display_name_ = "Whisper Transcription Service"
    _svc_description_ = "AIè¯­éŸ³è½¬å½•æœåŠ¡"
    
    def __init__(self, args):
        win32serviceutil.ServiceFramework.__init__(self, args)
        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)
        socket.setdefaulttimeout(60)
        self.model = WhisperModel("medium", device="cpu")
    
    def SvcStop(self):
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        win32event.SetEvent(self.hWaitStop)
    
    def SvcDoRun(self):
        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE,
                            servicemanager.PYS_SERVICE_STARTED,
                            (self._svc_name_, ''))
        self.main()
    
    def main(self):
        # å¯åŠ¨HTTPæœåŠ¡å™¨
        httpd = HTTPServer(('localhost', 8080), WhisperHandler)
        httpd.model = self.model
        httpd.serve_forever()

if __name__ == '__main__':
    win32serviceutil.HandleCommandLine(WhisperService)
```

### 2. é…ç½®æ–‡ä»¶ç®¡ç†

`whisper_config.json`:
```json
{
    "model": {
        "size": "medium",
        "device": "cpu",
        "compute_type": "int8"
    },
    "server": {
        "host": "localhost",
        "port": 8080,
        "max_file_size": "100MB"
    },
    "optimization": {
        "beam_size": 5,
        "vad_filter": true,
        "word_timestamps": true
    },
    "supported_formats": [".mp3", ".wav", ".flac", ".m4a", ".ogg"],
    "output_formats": ["json", "txt", "srt", "vtt"]
}
```

---

## ğŸ“š æ€»ç»“

Whisperåœ¨Windows 11ä¸Šçš„å®‰è£…å’Œä½¿ç”¨æ€»ç»“ï¼š

### âœ… æ¨èé…ç½®
- **Pythonç‰ˆæœ¬**ï¼š3.10æˆ–3.11
- **Whisperå®ç°**ï¼šFaster-Whisperï¼ˆæ€§èƒ½æ›´ä½³ï¼‰
- **é»˜è®¤æ¨¡å‹**ï¼šmediumï¼ˆå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰
- **è™šæ‹Ÿç¯å¢ƒ**ï¼šç‹¬ç«‹ç¯å¢ƒé¿å…å†²çª

### ğŸš€ æ€§èƒ½ä¼˜åŒ–è¦ç‚¹
1. ä½¿ç”¨Faster-Whisperæå‡4-5å€é€Ÿåº¦
2. æ˜ç¡®æŒ‡å®šéŸ³é¢‘è¯­è¨€
3. æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚æ¨¡å‹å¤§å°
4. æœ‰æ¡ä»¶æ—¶å¯ç”¨GPUåŠ é€Ÿ

### ğŸ”§ é›†æˆå»ºè®®
- ä½¿ç”¨Pythonè„šæœ¬é›†æˆåˆ°ç°æœ‰åº”ç”¨
- å®ç°æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡  
- é…ç½®åˆç†çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- è€ƒè™‘å†…å­˜ç®¡ç†å’Œæ€§èƒ½ç›‘æ§

è¿™ä»½æ‰‹å†Œæ¶µç›–äº†ä»åŸºç¡€å®‰è£…åˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„é…ç½®æ–¹æ¡ˆã€‚

---

*æ–‡æ¡£æ›´æ–°æ—¶é—´ï¼š2025-08-01*
*é€‚ç”¨ç‰ˆæœ¬ï¼šOpenAI Whisper Latest, Faster-Whisper Latest*